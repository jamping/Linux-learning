搭建步骤

第1-6步是每台服务器都需要的。

1. 升级系统，在命令行运行

yum update -y

2. 关闭防火墙,swap，因为k8s需要运行多个服务在不同的服务器上通讯，需要开放多个端口，简单起见，直接把防火墙关了，不推荐在生产环境这么做。关掉swap，k8s的组件kebelet才可以正常工作。

systemctl disable firewalld
systemctl stop firewalld

firewall-cmd --zone=public --add-port={6443/tcp,10250/tcp} --permanent && firewall-cmd --reload

禁用swap文件
然后需要禁用swap文件，这是Kubernetes的强制步骤。实现它很简单，编辑/etc/fstab文件，注释掉引用swap的行，保存并重启后输入sudo swapoff -a即可。
swapoff -a

3.安装docker
yum install -y docker-ce
systemctl enable docker && systemctl start docker

4.安装kubeadm,kubelet,kubectl

cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

yum install -y kubelet kubeadm kubectl
systemctl enable kubelet && systemctl start kubelet

5.关闭selinux，因为kubelet目前支持selinux还有点问题

setenforce 0  
sed -i 's/^SELINUX=.*/SELINUX=disabled/g' /etc/sysconfig/selinux

打开/etc/sysconfig/selinux文件

vi /etc/sysconfig/selinux  

找到SELINUX那行，改为
SELINUX=disabled
保存文件

6.设置net.bridge.bridge-nf-call-iptables为1

cat << EOF > /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sysctl --system

7.修改节点的 hostname，因为 kubernetes 是根据 hostname 来标示各节点的。

# Master 节点
echo "master.localdomain" > /etc/hostname
echo "10.236.65.125 master.localdomain" >> /etc/hosts
sysctl kernel.hostname=master.localdomain # 不重启情况下使内核修改生效

8. 修改配置文件
cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}
EOF

mkdir -p /etc/systemd/system/docker.service.d

# Restart Docker
systemctl daemon-reload
systemctl restart docker

vi /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
KUBELET_CGROUP_ARGS=cgroupfs

这里要特别强调一下,一定要修改kubelet 和 docker 的驱动为cgroupfs,修改后 reload 一下服务
systemctl daemon-reload

9.初始化master，在master的节点上运行

由于网络原因，我们需要提前拉取k8s初始化需要用到的Images，并添加对应的k8s.gcr.io标签
或者使用如下命令

kubeadm config images list |sed -e 's/^/docker pull /g' -e 's#k8s.gcr.io#docker.io/mirrorgooglecontainers#g' |sh -x
docker images |grep mirrorgooglecontainers |awk '{print "docker tag ",$1":"$2,$1":"$2}' |sed -e 's#docker.io/mirrorgooglecontainers#k8s.gcr.io#2' |sh -x
docker images |grep mirrorgooglecontainers |awk '{print "docker rmi ", $1":"$2}' |sh -x
docker pull coredns/coredns:1.2.2
docker tag coredns/coredns:1.2.2 k8s.gcr.io/coredns:1.2.2
docker rmi coredns/coredns:1.2.2

Master节点就是运行着控制组件的机器，包括etcd(集群数据库)和API服务(kubectl CLI通讯服务)。
初始化master节点, 只需随便在一台装过kubeadm的机器上运行如下命令

kubeadm init --kubernetes-version=v1.14.1 --feature-gates=CoreDNS=true --pod-network-cidr=192.168.0.0/16

把上面输出的最后一行 kubeadm join复制保存下来，后面在node节点加入到集群中需要用到
运行下面的命令初始化kebectl配置文件

To start using your cluster, you need to run the following as a regular user:

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

kubeadm join 192.168.0.117:6443 --token m8diet.2d2urrqiy4blpyli --discovery-token-ca-cert-hash sha256:084277f92c715524cc8e061411577b37f41cdd74da2b245973161af461eb9d9d
kubeadm join 192.168.0.201:6443 --token 0e1dt0.bcw3nrivtqqw2s0k --discovery-token-ca-cert-hash sha256:7f576e044fa65b866c6d4cb25f22a48b30b95e3c8a6e99ba579880b3cd9a5741

10.安装网络插件，以使pod能相互通讯，这里我们安装的是Calico.在master节点运行

kubectl apply -f https://docs.projectcalico.org/v3.0/getting-started/kubernetes/installation/hosted/kubeadm/1.7/calico.yaml

运行以下命令检查kube-dns pod 已经运行，一般需要几十秒

kubectl get pods --all-namespaces

如果输出中有名字以kube-dns的pod状态是Running，说明网络插件已经正常工作，然后就可以把node节点加入到集群


#!/bin/bash

version=v1.10.3
images=(kube-proxy-amd64:${version} kube-scheduler-amd64:${version} kube-controller-manager-amd64:${version} kube-apiserver-amd64:${version} etcd-amd64:3.2.18 pause-amd64:3.1 kubernetes-dashboard-amd64:v1.8.3 k8s-dns-sidecar-amd64:1.14.10 k8s-dns-kube-dns-amd64:1.14.10 k8s-dns-dnsmasq-nanny-amd64:1.14.10)

for imageName in ${images[@]} ; do

  docker pull jjp/$imageName
  docker tag jjp/$imageName k8s.gcr.io/$imageName
  docker rmi jjp/$imageName

done

11. 隔离主节点

默认情况下，出于安全的考虑，并不会在主节点上运行pod，如果你想在主节点上运行pod，比如：运行一个单机版的kubernetes集群时，可运行下面的命令：

kubectl taint nodes --all node-role.kubernetes.io/master-

(可选)在非主节点上管理集群
为了可以在其他电脑上使用kubectl来管理你的集群，可以从主节点上复制管理员 的kubeconfig文件到你的电脑上：

scp root@<master ip>:/etc/kubernetes/admin.conf .
kubectl --kubeconfig ./admin.conf get nodes

(可选)映射API服务到本地
如果你想从集群外部连接到API服务，可以使用工具kubectl proxy:

scp root@<master ip>:/etc/kubernetes/admin.conf .
kubectl --kubeconfig ./admin.conf proxy

这样就可以在本地这样 http://localhost:8001/api/v1 访问到API服务了。

(可选)部署一个微服务
现在可以测试你新搭建的集群了，Sock Shop就是一个微服务的样本，它体现了在Kubernetes里如何运行和连接一系列的服务。想了解更多关于微服务的内容，请查看GitHub README。

kubectl create namespace sock-shop
kubectl apply -n sock-shop -f "https://github.com/microservices-demo/microservices-demo/blob/master/deploy/kubernetes/complete-demo.yaml?raw=true"

可以通过以下命令来查看前端服务是否有开放对应的端口：
kubectl -n sock-shop get svc front-end

可能需要几分钟时间来下载和启用所有的容器，通过kubectl get pods -n sock-shop来获取服务的状态。

卸载socks shop, 只需要在主节点上运行:
kubectl delete namespace sock-shop

卸载集群
想要撤销kubeadm做的事，首先要排除节点，并确保在关闭节点之前要清空节点。
在主节点上运行：

kubectl drain <node name> --delete-local-data --force --ignore-daemonsets
kubectl delete node <node name>

然后在需要移除的节点上，重置kubeadm的安装状态：

kubeadm reset

如果你想重新配置集群，只需运行kubeadm init或者kubeadm join并使用所需的参数即可。

12. 安装kubernates-dashboard

1)生成证书

openssl req -newkey rsa:4096 -nodes -sha256 -keyout ca.key -x509 -days 365 -out ca.crt
openssl req -newkey rsa:4096 -nodes -sha256 -keyout weishuichao.key -out weishuichao.csr
openssl x509 -req -in weishuichao.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out weishuichao.crt

2)导入证书到kube-system

kubectl create secret generic kubernetes-dashboard-certs --from-file=/data/cert -n kube-system

3)创建kubernetes-dashboard.yaml

kubectl apply -f /data/k8s/kubernetes-dashboard.yaml

配置在官网的基础上需要加上下面配置ClusterRoleBinding，用以配置权限，这样通过token登陆进去才不至于没有权限

# ------------------- Dashboard ClusterRoleBinding ------------------- #

kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
name: kubernetes-dashboard
subjects:
- kind: ServiceAccount
name: kubernetes-dashboard
namespace: kube-system
roleRef:
kind: ClusterRole
name: cluster-admin
apiGroup: rbac.authorization.k8s.io

kubernetes采用暴露端口的方式使用，增加一下配置

4) 获取kube-system 的secret

kubectl -n kube-system get secret

5) 查看token,这个token 是用于登陆使用的

kubectl -n kube-system describe secret kubernetes-dashboard-token-2qmtr

6) 访问dashboard

https://192.168.2.117:31000 输入令牌，进入dashboard主页

13. 建立机器互信

1) 在每台服务器需要建立主机互信的用户名执行以下命令生成公钥/密钥，默认回车即可

$ ssh-keygen -t rsa

Generating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa):Created directory '/root/.ssh'.Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:0c:1f:76:aa:80:b2:2f:b9:9a:6a:48:96:9d:8f:a1:cc root@cctvyyycns04The key's randomart image is:+--[ RSA 2048]----+| || || . o . || . = + ||. + o S || * + . . ||B.. + . ||+E . . ||Oo. |+-----------------+

2) 互传公钥，第一次需要输入密码，之后就OK了

$ ssh-copy-id -i /root/.ssh/id_rsa.pub root@192.168.0.110

可以看到是在.ssh/下生成了个authorized_keys的文件，记录了能登陆这台服务器的其他服务器的公钥

3) 测试是否能登陆

$ ssh 192.168.0.110

14. 生成join token

kubeadm token create --print-join-command

把Node加入到master中去

kubeadm join 192.168.0.117:6443 --token c8jsl8.2noprwx5ljvezyyb --discovery-token-ca-cert-hash sha256:084277f92c715524cc8e061411577b37f41cdd74da2b245973161af461eb9d9d

